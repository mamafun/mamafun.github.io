
[
  
    {
      "title"    : "Home",
      "url"      : "/",
      "content"  : ""
    },
    
    {
      "title"    : "Installation Memo",
      "url"      : "/installation.html",
      "content"  : ""
    },
    
    {
      "title"    : "Kubernetes",
      "url"      : "/kubernetes.html",
      "content"  : ""
    },
    
    {
      "title"    : "Openshift 3.11",
      "url"      : "/openshift-3.11.html",
      "content"  : "## Environment\n\n- rhel-server-7.6-x86_64-dvd.iso\n- Bare-metal machine x1\n\n## Reference\n\n- https://docs.okd.io/latest/install/index.html\n\n## Deployment\n\n### Ensuring host access\n\n1. Generate an SSH key on the host you run the installation playbook on:\n```\n# ssh-keygen\n```\n\n2. Distribute the key to localhost\n```\n# ssh-copy-id -i ~/.ssh/id_rsa.pub localhost\n```\n\n### Installing base packages\n\n1. Install the following base packages:\n```\n# yum install wget git net-tools bind-utils yum-utils iptables-services bridge-utils bash-completion kexec-tools sos psacct\n```\n\n2. (install with RPM-based installer) Install Ansible. To use EPEL as a package source for Ansible.\n    1. Install the EPEL repository:\n    ```\n     # yum -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n    ```\n\n    2. Disable the EPEL repository globally so that it is not accidentally used during later steps of the installation:\n    ```\n    # sed -i -e \"s/^enabled=1/enabled=0/\" /etc/yum.repos.d/epel.repo\n    ```\n\n    3. Install the packages for Ansible:\n    ```\n    # yum -y --enablerepo=epel install ansible pyOpenSSL\n    ```\n\n3. Clone the openshift/openshift-ansible repository from GitHub, which provides the required playbooks and configuration files:\n```\n# cd ~\n# git clone https://github.com/openshift/openshift-ansible\n# cd openshift-ansible\n# git checkout release-3.11\n```\n\n### Installing Docker\n\n1. Install Docker 1.13:\n```\n# subscription-manager repos --enable=rhel-7-server-extras-rpms\n# yum install docker-1.13.1\n# systemctl start docker\n```\n\n2. Verify that version 1.13 was installed:\n```\n# rpm -V docker-1.13.1\n# docker version\n```\n\n### Installing OKD (Running the RPM-based installer)\n\n1. Change to the playbook directory and run the prerequisites.yml playbook. This playbook installs required software packages, if any, and modifies the container runtimes. Unless you need to configure the container runtimes, run this playbook only once, before you deploy a cluster the first time:\n```\n# cd /root/openshift-ansible\n# ansible-playbook -i inventory/hosts.localhost playbooks/prerequisites.yml\n```\n\n2. Initiate the cluster installation:\n```\n# ansible-playbook -i inventory/hosts.localhost playbooks/deploy_cluster.yml\n```\n\n### Verifying the Installation\n\n1. Verify that the master is started and nodes are registered and reporting in Ready status. On the master host, run the following command as root:\n```\n# oc get nodes\nNAME                    STATUS    ROLES                  AGE       VERSION\nlocalhost.localdomain   Ready     compute,infra,master   14m       v1.11.0+d4cacc0\n```\n\n2. To verify that the web console is installed correctly, use the master host name and the web console port number to access the web console with a web browser.\n\nFor example, for a master host with a host name of localhost and using the default port of 8443, the web console URL is https://localhost:8443/console.\n\n### Uninstalling an OKD cluster\n\nTo uninstall OKD across all hosts in your cluster, change to the playbook directory and run the playbook using the inventory file you used most recently:\n```\n# ansible-playbook -i inventory/hosts.localhost playbooks/adhoc/uninstall.yml\n```\n\n"
    },
    
    {
      "title"    : "Openshift 4.0 (Draft)",
      "url"      : "/openshift-4.0.html",
      "content"  : "(Draft)\n\n## Environment\n\n- rhel-server-7.6-x86_64-dvd.iso\n- Bare-metal machine x1\n\n## Reference\n\n- https://docs.okd.io/latest/install/index.html\n\n## Deployment\n\n### Ensuring host access\n\n1. Generate an SSH key on the host you run the installation playbook on:\n```\n# ssh-keygen\n```\n\n2. Distribute the key to localhost\n```\n# ssh-copy-id -i ~/.ssh/id_rsa.pub localhost\n```\n\n### Installing base packages\n\n1. Install the following base packages:\n```\n# yum install wget git net-tools bind-utils yum-utils iptables-services bridge-utils bash-completion kexec-tools sos psacct\n```\n\n2. (install with RPM-based installer) Install Ansible. To use EPEL as a package source for Ansible.\n    1. Install the EPEL repository:\n    ```\n     # yum -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n    ```\n\n    2. Disable the EPEL repository globally so that it is not accidentally used during later steps of the installation:\n    ```\n    # sed -i -e \"s/^enabled=1/enabled=0/\" /etc/yum.repos.d/epel.repo\n    ```\n\n    3. Install the packages for Ansible:\n    ```\n    # yum -y --enablerepo=epel install ansible pyOpenSSL\n    ```\n\n3. Clone the openshift/openshift-ansible repository from GitHub, which provides the required playbooks and configuration files:\n```\n# cd ~\n# git clone https://github.com/openshift/openshift-ansible\n# cd openshift-ansible\n# git checkout release-3.11\n```\n\n### Installing Docker\n\n1. Install Docker 1.13:\n```\n# subscription-manager repos --enable=rhel-7-server-extras-rpms\n# yum install docker-1.13.1\n# systemctl start docker\n```\n\n2. Verify that version 1.13 was installed:\n```\n# rpm -V docker-1.13.1\n# docker version\n```\n\n### Installing OKD (Running the RPM-based installer)\n\n```\ncd /etc/yum.repos.d && curl -O https://storage.googleapis.com/origin-ci-test/releases/openshift/origin/master/origin.repo\n```\n\n1. Change to the playbook directory and run the prerequisites.yml playbook. This playbook installs required software packages, if any, and modifies the container runtimes. Unless you need to configure the container runtimes, run this playbook only once, before you deploy a cluster the first time:\n```\n# cd /root/openshift-ansible\n# ansible-playbook -i inventory/hosts.localhost playbooks/prerequisites.yml\n```\n\n2. Initiate the cluster installation:\n```\n# ansible-playbook -i inventory/hosts.localhost playbooks/deploy_cluster.yml\n```\n\n### Verifying the Installation\n\n1. Verify that the master is started and nodes are registered and reporting in Ready status. On the master host, run the following command as root:\n```\n# oc get nodes\nNAME                    STATUS    ROLES                  AGE       VERSION\nlocalhost.localdomain   Ready     compute,infra,master   14m       v1.11.0+d4cacc0\n```\n\n2. To verify that the web console is installed correctly, use the master host name and the web console port number to access the web console with a web browser.\n\nFor example, for a master host with a host name of localhost and using the default port of 8443, the web console URL is https://localhost:8443/console.\n\n### Uninstalling an OKD cluster\n\nTo uninstall OKD across all hosts in your cluster, change to the playbook directory and run the playbook using the inventory file you used most recently:\n```\n# ansible-playbook -i inventory/hosts.localhost playbooks/adhoc/uninstall.yml\n```\n\n"
    },
    
    {
      "title"    : "Operator Framework",
      "url"      : "/operator-framework.html",
      "content"  : "## Background\n\n### OLM Architecture\n\n[OLM Architecture](https://github.com/operator-framework/operator-lifecycle-manager/blob/274df58592c2ffd1d8ea56156c73c7746f57efc0/Documentation/design/architecture.md)\n\n### Operator Framework Object Map\n![object map](./img/obj-map.png)\n\n## Deploy Operator Framework\n\n### Deploy OLM and Catalog Operator\n\n```\n$ git clone https://github.com/operator-framework/operator-lifecycle-manager.git\n$ sed -i 's/image: quay.io\\/coreos\\/olm@.*/image: quay.io\\/openshift\\/origin-operator-lifecycle-manager:latest/g' ./deploy/upstream/manifests/latest/*\n$ kubectl create -f deploy/upstream/manifests/latest/\n```\nRefer to [Install document](https://github.com/operator-framework/operator-lifecycle-manager/blob/master/Documentation/install/install.md) for details.\n\n### Deploy Marketplace Operator\n\n```\n$ git clone https://github.com/operator-framework/operator-marketplace.git\n$ kubectl apply -f deploy/upstream\n```\nRefer to [README.md](https://github.com/operator-framework/operator-marketplace/blob/master/README.md) for details.\n\n\n### Visualize OperatorHub in OKD 3.11\n\n1. Prepare an OKD 3.11 cluster\n2. Edit openshift-console deployment to replace ```image: docker.io/openshift/origin-console:v3.11``` to ```image: docker.io/openshift/origin-console:v4.0.0```\n```\n$ oc edit pod console-67dd586f67-7bsxt -n openshift-console\n```\n3. login to openshift GUI at ```/operatorhub/all-namespaces```\n\n## Workflow\n\n1. Operator developer prepares all [CSV](https://github.com/operator-framework/operator-lifecycle-manager/blob/master/Documentation/design/building-your-csv.md), CRD, and Package yamls.\n2. Operator developer uses [```operator-courier```](https://github.com/operator-framework/operator-courier/#usage) to verify and push operator bundle to the Quay application repository.\n3. Cluster admin creates [```OperatorSource```](https://github.com/operator-framework/operator-marketplace#description) CR to add the operator source into cluster. (All operators under the same Quay namespace share the same ```OperatorSource```)\n4. Cluster admin creates [```CatalogSourceConfig```](https://github.com/operator-framework/operator-marketplace/blob/master/README.md#description) to enable selected operators in ```OperatorSource``` to ```CatalogSource```\n5. Cluster admin creates [```OperatorGroup```](https://github.com/operator-framework/operator-lifecycle-manager/blob/master/Documentation/design/operatorgroups.md) for target namespaces where operators are going to be installed into.\n6. Cluster admin creates ```Subscription``` to subscribe operators.\n7. *Catalog Operator* will reconcile to ```Subscription``` and create ```InstallPlan```.\n8. *Catalog Operator* will reconcile to ```InstallPlan``` and create corresponding ```ClusterServiceVersion```.\n9. *OLM Operator* reconciles to ```ClusterServiceVersion``` and make operators started.\n10. User creates CRs defined by operators to create services.\n\n### Examples\n\n1. Import **operatorsource** provided by Federator.ai\n    ```\n    $ kubectl apply -f prophetstor-operatorsource.yaml\n    $ cat prophetstor-operatorsource.yaml\n    apiVersion: operators.coreos.com/v1\n    kind: OperatorSource\n    metadata:\n      name: prophetstor-operators\n      namespace: openshift-marketplace\n    spec:\n      type: appregistry\n      endpoint: https://quay.io/cnr\n      registryNamespace: prophetstor\n      displayName: \"prophetstor Operators\"\n      publisher: \"prophetstor\"\n    ```\n\n    You should see the FederatorAI Operator in the UI.\n    ![operatorhub](./img/operatorsource.png)\n\n2. Subscribe Federator.ai operator\n    ![subscribe](./img/subscribeoperator.png)\n\n3. Configure namespace and approval strategy\n    ![config subscription](./img/subscribesetting.png)\n\n4. (Optional) Approve Federator.ai operator **InstallPlan** if you toggle *Manual* approval strategy when you were subscribing Federator.ai operator\n\n5. Check subscription status\n    ![subscription status](./img/subscribestatus.png)\n\n6. Check installed ClusterServiceVersion\n    ![csv](./img/csv.png)\n\n7. When Federator.ai **ClusterServiceVersion** of Federator.ai operator is created, we can create an **AlamedaService** CR to start deploy Alameda\n    ![create alamedaservice](./img/createcr.png)\n\n"
    },
    {}
]

