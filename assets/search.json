
[
  
    {
      "title"    : "Federator.ai v4.2",
      "url"      : "/federator.ai-4.2.html",
      "content"  : "## Objective\n\n- Install Federator.ai v4.2\n- Create an Alamedascaler to watch an application\n\n## Requirement\n\n- Kubernetes 1.11+\n- Prometheus  \n  Checkout the document of required metrics at [required metrics for Alameda](https://github.com/containers-ai/alameda/blob/master/docs/metrics_used_in_Alameda.md)\n\n## Reference\n\n- [Federator.ai Operator](https://github.com/containers-ai/federatorai-operator)\n- [Required metrics for Alameda](https://github.com/containers-ai/alameda/blob/master/docs/metrics_used_in_Alameda.md)\n- [AlamedaScaler](https://github.com/containers-ai/alameda/blob/master/design/crd_alamedascaler.md)\n\n## Install\n\n1. Checkout the latest tag number of v4.2. Assuming ```v4.2.159```. Execute the following command and follow the interactive prompts to input settings.\n\n```\n# curl https://raw.githubusercontent.com/containers-ai/federatorai-operator/4.2/deploy/install.sh |sh -s v4.2.159\nPlease input Federator.ai Operator tag: v4.2.159\nIs tag \"v4.2.159\" correct? [default: y]:\nDownloading file 00-namespace.yaml ...\nDone\nDownloading file 01-serviceaccount.yaml ...\nDone\nDownloading file 02-alamedaservice.crd.yaml ...\nDone\nDownloading file 03-federatorai-operator.deployment.yaml ...\nDone\nDownloading file 04-clusterrole.yaml ...\nDone\nDownloading file 05-clusterrolebinding.yaml ...\nDone\nDownloading file 06-role.yaml ...\nDone\nDownloading file 07-rolebinding.yaml ...\nDone\n\nStarting apply Federator.ai operator yaml files\nnamespace/federatorai created\nserviceaccount/federatorai-operator created\ncustomresourcedefinition.apiextensions.k8s.io/alamedaservices.federatorai.containers.ai created\ndeployment.apps/federatorai-operator created\nclusterrole.rbac.authorization.k8s.io/federatorai-operator created\nclusterrolebinding.rbac.authorization.k8s.io/federatorai-operator created\nrole.rbac.authorization.k8s.io/federatorai-operator created\nrolebinding.rbac.authorization.k8s.io/federatorai-operator created\nProcessing...\nWaiting for federatorai pods to be ready...\n\nAll federatorai pods are ready.\n\nInstall Federator.ai operator v4.2.159 successfully\n\nDownloading alamedaservice and alamedascaler sample files ...\nDone\n========================================\nDo you want to launch interactive installation of Federator.ai [default: y]:\nEnter the namespace you want to install Federator.ai [default: alameda]:\nDo you want to enable execution? [default: y]: :\nEnter the prometheus service address\n[default: https://prometheus-k8s.openshift-monitoring:9091]: http://prometheus-prometheus-oper-prometheus.monitoring:9090\nWhich storage type you would like to use? ephemeral or persistent?\n[default: ephemeral]:\n\n----------------------------------------\ninstall_namespace = alameda\nenable_execution = true\nprometheus_address = http://prometheus-prometheus-oper-prometheus.monitoring:9090\nstorage_type = ephemeral\n----------------------------------------\nIs the above information correct [default: y]:\nProcessing...\nWaiting for alameda pods to be ready...\nWaiting for alameda pods to be ready...\nWaiting for alameda pods to be ready...\nWaiting for alameda pods to be ready...\nWaiting for alameda pods to be ready...\nWaiting for alameda pods to be ready...\nWaiting for alameda pods to be ready...\nWaiting for alameda pods to be ready...\nWaiting for alameda pods to be ready...\nWaiting for alameda pods to be ready...\n\nAll alameda pods are ready.\n\nInstall Alameda v4.2.159 successfully\n\nDownloaded YAML files are located under /tmp/install-op\n```\n\n- Check if we have all the pods of federator.ai running by:\n\n```\n[root@localhost ~]# kubectl get pods --all-namespaces\nNAMESPACE       NAME                                                     READY   STATUS    RESTARTS   AGE\nalameda         admission-controller-69f69d845c-d5nlj                    1/1     Running   0          13m\nalameda         alameda-ai-58574cbbb8-kkfqs                              1/1     Running   0          17m\nalameda         alameda-datahub-dcbd7887-n4nd4                           1/1     Running   0          17m\nalameda         alameda-evictioner-86bbd47d48-lzd8b                      1/1     Running   0          17m\nalameda         alameda-executor-64dbddf9d-xpp2h                         1/1     Running   0          17m\nalameda         alameda-grafana-56cb58699b-z7k2z                         1/1     Running   0          17m\nalameda         alameda-influxdb-777488c846-j6mpq                        1/1     Running   0          17m\nalameda         alameda-operator-7688d6b48b-gngdq                        1/1     Running   0          14m\nalameda         alameda-recommender-5b885f8698-dwl7p                     1/1     Running   0          17m\nfederatorai     federatorai-operator-5d985d98bc-44qq6                    1/1     Running   0          20m\nkube-system     coredns-5c98db65d4-t87r4                                 1/1     Running   0          27m\nkube-system     coredns-5c98db65d4-whx4k                                 1/1     Running   0          27m\nkube-system     etcd-localhost.localdomain                               1/1     Running   0          26m\nkube-system     kube-apiserver-localhost.localdomain                     1/1     Running   0          26m\nkube-system     kube-controller-manager-localhost.localdomain            1/1     Running   3          26m\nkube-system     kube-flannel-ds-amd64-hqxxd                              1/1     Running   0          27m\nkube-system     kube-proxy-zl4md                                         1/1     Running   0          27m\nkube-system     kube-scheduler-localhost.localdomain                     1/1     Running   3          27m\nkube-system     tiller-deploy-58565b5464-mcg89                           1/1     Running   0          25m\nmonitoring      alertmanager-prometheus-prometheus-oper-alertmanager-0   2/2     Running   0          23m\nmonitoring      prometheus-grafana-7bd5f968b5-742zt                      2/2     Running   0          23m\nmonitoring      prometheus-kube-state-metrics-5f7884cf-bczrg             1/1     Running   0          23m\nmonitoring      prometheus-prometheus-node-exporter-k8n7b                1/1     Running   0          23m\nmonitoring      prometheus-prometheus-oper-operator-6ff596b864-tjd74     1/1     Running   0          23m\nmonitoring      prometheus-prometheus-prometheus-oper-prometheus-0       3/3     Running   0          23m\n```\n\nIt should have the 10 pods in alameda namespace and the pod in federatorai namespace.\n\n## Create an AlamedaScaler\n\n- Create a yaml file ```alamedascaler.yaml``` with the following content.\n\n```\napiVersion: autoscaling.containers.ai/v1alpha1\nkind: AlamedaScaler\nmetadata:\n  name: alamedaapp\n  namespace: alameda\nspec:\n  policy: stable\n  enableExecution: false\n  scalingTool:\n    type: vpa\n  selector:\n    matchLabels:\n      app: alameda\n```\n\n- Apply file ```alamedascaler.yaml``` with kubectl command.\n\n```\n# kubectl apply -f alamedascaler.yaml\n```\n\n- Checkout what resource objects are being watched by Alameda\n\nThe resource objects listed under ```status``` key are what being watched.\n\n```\n# kubectl get alamedascaler -n alameda -o yaml\napiVersion: v1\nitems:\n- apiVersion: autoscaling.containers.ai/v1alpha1\n  kind: AlamedaScaler\n  metadata:\n    creationTimestamp: \"2019-08-22T03:56:45Z\"\n    generation: 2\n    name: alamedaapp\n    namespace: alameda\n    resourceVersion: \"4088\"\n    selfLink: /apis/autoscaling.containers.ai/v1alpha1/namespaces/alameda/alamedascalers/alamedaapp\n    uid: d55d76fa-5872-4099-99ea-6847a5b32692\n  spec:\n    enableExecution: false\n    policy: stable\n    scalingTool:\n      executionStrategy:\n        maxUnavailable: 25%\n        triggerThreshold:\n          cpu: 10%\n          memory: 10%\n      type: vpa\n    selector:\n      matchLabels:\n        app: alameda\n  status:\n    alamedaController:\n      deployments:\n        alameda/admission-controller:\n          name: admission-controller\n          namespace: alameda\n          pods:\n            alameda/admission-controller-69f69d845c-d5nlj:\n              containers:\n              - name: admission-controller\n                resources: {}\n              name: admission-controller-69f69d845c-d5nlj\n              namespace: alameda\n              uid: abb80d4f-b52f-467a-9176-431bce560460\n          specReplicas: 1\n          uid: d8b7d42e-abc5-4033-96cc-f3446057c49f\n        alameda/alameda-ai:\n          name: alameda-ai\n          namespace: alameda\n          pods:\n            alameda/alameda-ai-58574cbbb8-kkfqs:\n              containers:\n              - name: alameda-ai-engine\n                resources: {}\n              name: alameda-ai-58574cbbb8-kkfqs\n              namespace: alameda\n              uid: 2912ae24-d3de-493d-95b2-849cbf5500a7\n          specReplicas: 1\n          uid: d33ab874-6aef-4d62-8eb9-72771ec65fd6\n        alameda/alameda-datahub:\n          name: alameda-datahub\n          namespace: alameda\n          pods:\n            alameda/alameda-datahub-dcbd7887-n4nd4:\n              containers:\n              - name: alameda-datahub\n                resources: {}\n              name: alameda-datahub-dcbd7887-n4nd4\n              namespace: alameda\n              uid: 674fdcc6-6222-48f1-b6de-ebd57ce39e61\n          specReplicas: 1\n          uid: cea14d4e-e84a-4ad8-9708-7f622fc66f13\n        alameda/alameda-evictioner:\n          name: alameda-evictioner\n          namespace: alameda\n          pods:\n            alameda/alameda-evictioner-86bbd47d48-lzd8b:\n              containers:\n              - name: alameda-evictioner\n                resources: {}\n              name: alameda-evictioner-86bbd47d48-lzd8b\n              namespace: alameda\n              uid: add80856-7702-41de-ae6d-3c43d1cb71d9\n          specReplicas: 1\n          uid: 813e47b3-7a47-4fed-afc0-7c4648e6218e\n        alameda/alameda-executor:\n          name: alameda-executor\n          namespace: alameda\n          pods:\n            alameda/alameda-executor-64dbddf9d-xpp2h:\n              containers:\n              - name: alameda-executor\n                resources: {}\n              name: alameda-executor-64dbddf9d-xpp2h\n              namespace: alameda\n              uid: beb80163-d746-4a17-aca8-e59e643ce692\n          specReplicas: 1\n          uid: f604dde3-e569-47ac-95b2-a2c9063c9eb0\n        alameda/alameda-grafana:\n          name: alameda-grafana\n          namespace: alameda\n          pods:\n            alameda/alameda-grafana-56cb58699b-z7k2z:\n              containers:\n              - name: grafana\n                resources: {}\n              name: alameda-grafana-56cb58699b-z7k2z\n              namespace: alameda\n              uid: e772c10d-36cc-4288-a934-1d978cf05235\n          specReplicas: 1\n          uid: da29e058-2d5a-4b62-a5f6-765b51fa876c\n        alameda/alameda-influxdb:\n          name: alameda-influxdb\n          namespace: alameda\n          pods:\n            alameda/alameda-influxdb-777488c846-j6mpq:\n              containers:\n              - name: influxdb\n                resources: {}\n              name: alameda-influxdb-777488c846-j6mpq\n              namespace: alameda\n              uid: 07b1fa38-1121-4203-95b6-7ffc79c25058\n          specReplicas: 1\n          uid: 1299fcb9-8e05-4387-b9c5-d7a4f778348b\n        alameda/alameda-operator:\n          name: alameda-operator\n          namespace: alameda\n          pods:\n            alameda/alameda-operator-7688d6b48b-gngdq:\n              containers:\n              - name: alameda-operator\n                resources: {}\n              name: alameda-operator-7688d6b48b-gngdq\n              namespace: alameda\n              uid: 2976e584-1c14-4f40-8a21-877994a84d11\n          specReplicas: 1\n          uid: e0c451a2-7533-4e89-a47f-25c63ba10786\n        alameda/alameda-recommender:\n          name: alameda-recommender\n          namespace: alameda\n          pods:\n            alameda/alameda-recommender-5b885f8698-dwl7p:\n              containers:\n              - name: alameda-recommender\n                resources: {}\n              name: alameda-recommender-5b885f8698-dwl7p\n              namespace: alameda\n              uid: 30497675-e450-4e53-ae08-08779bad393e\n          specReplicas: 1\n          uid: 1a08f87c-142c-4cf0-97ce-8831dc7ad46a\nkind: List\nmetadata:\n  resourceVersion: \"\"\n  selfLink: \"\"\n```\n\n- Checkout the Alameda Grafana dashboard\n\n```\n# kubectl patch service alameda-grafana -n alameda --patch '{\"spec\": {\"type\": \"NodePort\"}}'\n# port=`kubectl get service alameda-grafana -n alameda -o=jsonpath='{.spec.ports[*].nodePort}'`\n# echo $port\n```\n\nOpen a browser to enter an URL with your machine's IP and $port. Checkout ```VPA Resource Recommendations per Container``` dashboard should show the alameda application.\n\n![federator.ai-vpa-dashboard](./img/federator-vpa-dashboard.png)\n\n"
    },
    
    {
      "title"    : "Home",
      "url"      : "/",
      "content"  : ""
    },
    
    {
      "title"    : "Installation Memo",
      "url"      : "/installation.html",
      "content"  : ""
    },
    
    {
      "title"    : "Kubernetes",
      "url"      : "/kubernetes.html",
      "content"  : "# CentOS & RHEL\n\n```\n#!/usr/bin/bash\n\n#\niptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X\n\n#\ncat  /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\nexclude=kube*\nEOF\n\n# Set SELinux in permissive mode (effectively disabling it)\nsetenforce 0\nsed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config\n\nyum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes\n\n#\nsystemctl enable --now kubelet\n\ncat   /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nEOF\nsysctl --system\n\n# \nkubeadm init --pod-network-cidr=10.244.0.0/16\n\n#\nmkdir -p $HOME/.kube\nsudo cp -f /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\n#\nkubectl taint nodes --all node-role.kubernetes.io/master-\n\n# \n#sysctl net.bridge.bridge-nf-call-iptables=1\n#kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\n\n```\n"
    },
    
    {
      "title"    : "Openshift 3.11",
      "url"      : "/openshift-3.11.html",
      "content"  : "## Environment\n\n- rhel-server-7.6-x86_64-dvd.iso\n- Bare-metal machine x1\n\n## Reference\n\n- https://docs.okd.io/latest/install/index.html\n\n## Deployment\n\n### Ensuring host access\n\n1. Generate an SSH key on the host you run the installation playbook on:\n```\n# ssh-keygen\n```\n\n2. Distribute the key to localhost\n```\n# ssh-copy-id -i ~/.ssh/id_rsa.pub localhost\n```\n\n### Installing base packages\n\n1. Install the following base packages:\n```\n# yum install wget git net-tools bind-utils yum-utils iptables-services bridge-utils bash-completion kexec-tools sos psacct\n```\n\n2. (install with RPM-based installer) Install Ansible. To use EPEL as a package source for Ansible.\n    1. Install the EPEL repository:\n    ```\n     # yum -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n    ```\n\n    2. Disable the EPEL repository globally so that it is not accidentally used during later steps of the installation:\n    ```\n    # sed -i -e \"s/^enabled=1/enabled=0/\" /etc/yum.repos.d/epel.repo\n    ```\n\n    3. Install the packages for Ansible:\n    ```\n    # yum -y --enablerepo=epel install ansible pyOpenSSL\n    ```\n\n3. Clone the openshift/openshift-ansible repository from GitHub, which provides the required playbooks and configuration files:\n```\n# cd ~\n# git clone -b release-3.11 https://github.com/openshift/openshift-ansible\n# cd openshift-ansible\n```\n\n### Installing Docker\n\n1. Install Docker 1.13:\n```\n# subscription-manager repos --enable=rhel-7-server-extras-rpms\n# yum install docker-1.13.1\n# systemctl start docker\n```\n\n2. Verify that version 1.13 was installed:\n```\n# rpm -V docker-1.13.1\n# docker version\n```\n\n### Installing OKD (Running the RPM-based installer)\n\n1. Change to the playbook directory and run the prerequisites.yml playbook. This playbook installs required software packages, if any, and modifies the container runtimes. Unless you need to configure the container runtimes, run this playbook only once, before you deploy a cluster the first time:\n```\n# cd /root/openshift-ansible\n# ansible-playbook -i inventory/hosts.localhost playbooks/prerequisites.yml\n```\n\n2. Initiate the cluster installation:\n```\n# ansible-playbook -i inventory/hosts.localhost playbooks/deploy_cluster.yml\n```\n\n> **Note:** add the following directives for using hostname other than localhost  \nopenshift_master_cluster_hostname=okd-internal.\\.nip.io  \nopenshift_master_cluster_public_hostname=okd-public.\\.nip.io  \nopenshift_master_default_subdomain=apps.\\.nip.io  \n\n### Verifying the Installation\n\n1. Verify that the master is started and nodes are registered and reporting in Ready status. On the master host, run the following command as root:\n```\n# oc get nodes\nNAME                    STATUS    ROLES                  AGE       VERSION\nlocalhost.localdomain   Ready     compute,infra,master   14m       v1.11.0+d4cacc0\n```\n\n2. To verify that the web console is installed correctly, use the master host name and the web console port number to access the web console with a web browser.\n\nFor example, for a master host with a host name of localhost and using the default port of 8443, the web console URL is https://localhost:8443/console.\n\n### Uninstalling an OKD cluster\n\nTo uninstall OKD across all hosts in your cluster, change to the playbook directory and run the playbook using the inventory file you used most recently:\n```\n# ansible-playbook -i inventory/hosts.localhost playbooks/adhoc/uninstall.yml\n```\n\n"
    },
    
    {
      "title"    : "Openshift 3.9",
      "url"      : "/openshift-3.9.html",
      "content"  : "## Environment\n\n- rhel-server-7.6-x86_64-dvd.iso\n- Bare-metal machine x1\n\n## Reference\n\n- https://docs.okd.io/3.9/install_config/index.html\n\n## Deployment\n\n### Ensuring host access\n\n1. Generate an SSH key on the host you run the installation playbook on:\n```\n# ssh-keygen\n```\n\n2. Distribute the key to localhost\n```\n# ssh-copy-id -i ~/.ssh/id_rsa.pub localhost\n```\n\n### Installing base packages\n\n1. Install the following base packages:\n```\n# yum -y install wget git net-tools bind-utils yum-utils iptables-services bridge-utils bash-completion kexec-tools sos psacct\n```\n\n2. (install with RPM-based installer) Install Ansible. To use EPEL as a package source for Ansible.\n    1. download ansible 2.4.3.0 from https://releases.ansible.com/ansible/\n    ```\n    # wget https://releases.ansible.com/ansible/ansible-2.4.3.0.tar.gz\n    ```\n    2. Install ansible 2.4.3.0\n    ```\n    # tar zxvf ansible-2.4.3.0.tar.gz\n    # cd ansible-2.4.3.0/\n    # python setup.py install\n    ```\n\n3. Clone the openshift/openshift-ansible repository from GitHub, which provides the required playbooks and configuration files:\n```\n# cd ~\n# git clone -b release-3.9 --single-branch https://github.com/openshift/openshift-ansible\n# cd openshift-ansible\n```\n\n### Installing Docker\n\n1. Install Docker 1.13:\n```\n# subscription-manager repos --enable=rhel-7-server-extras-rpms\n# yum install docker-1.13.1\n# systemctl start docker\n```\n\n2. Verify that version 1.13 was installed:\n```\n# rpm -V docker-1.13.1\n# docker version\n```\n\n### Installing OKD (Running the RPM-based installer)\n\n1. Change to the playbook directory and run the prerequisites.yml playbook. This playbook installs required software packages, if any, and modifies the container runtimes. Unless you need to configure the container runtimes, run this playbook only once, before you deploy a cluster the first time:\n```\n# cd /root/openshift-ansible\n# ansible-playbook -i inventory/hosts.localhost playbooks/prerequisites.yml\n```\n\n2. Initiate the cluster installation:\n```\n# ansible-playbook -i inventory/hosts.localhost playbooks/deploy_cluster.yml\n```\n\n> **Note:** add the following directives for using hostname other than localhost  \nopenshift_master_cluster_hostname=okd-internal.\\.nip.io  \nopenshift_master_cluster_public_hostname=okd-public.\\.nip.io  \nopenshift_master_default_subdomain=apps.\\.nip.io  \n\n### Verifying the Installation\n\n1. Verify that the master is started and nodes are registered and reporting in Ready status. On the master host, run the following command as root:\n```\n# oc get nodes\nNAME                    STATUS    ROLES                  AGE       VERSION\nlocalhost.localdomain   Ready     compute,infra,master   14m       v1.9.1+a0ce1bc657\n```\n\n2. To verify that the web console is installed correctly, use the master host name and the web console port number to access the web console with a web browser.\n\nFor example, for a master host with a host name of localhost and using the default port of 8443, the web console URL is https://localhost:8443/console.\n\n### Uninstalling an OKD cluster\n\nTo uninstall OKD across all hosts in your cluster, change to the playbook directory and run the playbook using the inventory file you used most recently:\n```\n# ansible-playbook -i inventory/hosts.localhost playbooks/adhoc/uninstall.yml\n```\n\n"
    },
    
    {
      "title"    : "Openshift 4.0 (Draft)",
      "url"      : "/openshift-4.0.html",
      "content"  : "(Draft)\n\n## Environment\n\n- rhel-server-7.6-x86_64-dvd.iso\n- Bare-metal machine x1\n\n## Reference\n\n- https://docs.okd.io/latest/install/index.html\n\n## Deployment\n\n### Ensuring host access\n\n1. Generate an SSH key on the host you run the installation playbook on:\n```\n# ssh-keygen\n```\n\n2. Distribute the key to localhost\n```\n# ssh-copy-id -i ~/.ssh/id_rsa.pub localhost\n```\n\n### Installing base packages\n\n1. Install the following base packages:\n```\n# yum install wget git net-tools bind-utils yum-utils iptables-services bridge-utils bash-completion kexec-tools sos psacct\n```\n\n2. (install with RPM-based installer) Install Ansible. To use EPEL as a package source for Ansible.\n    1. Install the EPEL repository:\n    ```\n     # yum -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n    ```\n\n    2. Disable the EPEL repository globally so that it is not accidentally used during later steps of the installation:\n    ```\n    # sed -i -e \"s/^enabled=1/enabled=0/\" /etc/yum.repos.d/epel.repo\n    ```\n\n    3. Install the packages for Ansible:\n    ```\n    # yum -y --enablerepo=epel install ansible pyOpenSSL\n    ```\n\n3. Clone the openshift/openshift-ansible repository from GitHub, which provides the required playbooks and configuration files:\n```\n# cd ~\n# git clone https://github.com/openshift/openshift-ansible\n# cd openshift-ansible\n# git checkout release-3.11\n```\n\n### Installing Docker\n\n1. Install Docker 1.13:\n```\n# subscription-manager repos --enable=rhel-7-server-extras-rpms\n# yum install docker-1.13.1\n# systemctl start docker\n```\n\n2. Verify that version 1.13 was installed:\n```\n# rpm -V docker-1.13.1\n# docker version\n```\n\n### Installing OKD (Running the RPM-based installer)\n\n```\ncd /etc/yum.repos.d && curl -O https://storage.googleapis.com/origin-ci-test/releases/openshift/origin/master/origin.repo\n```\n\n1. Change to the playbook directory and run the prerequisites.yml playbook. This playbook installs required software packages, if any, and modifies the container runtimes. Unless you need to configure the container runtimes, run this playbook only once, before you deploy a cluster the first time:\n```\n# cd /root/openshift-ansible\n# ansible-playbook -i inventory/hosts.localhost playbooks/prerequisites.yml\n```\n\n2. Initiate the cluster installation:\n```\n# ansible-playbook -i inventory/hosts.localhost playbooks/deploy_cluster.yml\n```\n\n### Verifying the Installation\n\n1. Verify that the master is started and nodes are registered and reporting in Ready status. On the master host, run the following command as root:\n```\n# oc get nodes\nNAME                    STATUS    ROLES                  AGE       VERSION\nlocalhost.localdomain   Ready     compute,infra,master   14m       v1.11.0+d4cacc0\n```\n\n2. To verify that the web console is installed correctly, use the master host name and the web console port number to access the web console with a web browser.\n\nFor example, for a master host with a host name of localhost and using the default port of 8443, the web console URL is https://localhost:8443/console.\n\n### Uninstalling an OKD cluster\n\nTo uninstall OKD across all hosts in your cluster, change to the playbook directory and run the playbook using the inventory file you used most recently:\n```\n# ansible-playbook -i inventory/hosts.localhost playbooks/adhoc/uninstall.yml\n```\n\n"
    },
    
    {
      "title"    : "Operator Framework",
      "url"      : "/operator-framework.html",
      "content"  : "## Background\n\n### OLM Architecture\n\n[OLM Architecture](https://github.com/operator-framework/operator-lifecycle-manager/blob/274df58592c2ffd1d8ea56156c73c7746f57efc0/Documentation/design/architecture.md)\n\n### Operator Framework Object Map\n![object map](./img/obj-map.png)\n\n## Deploy Operator Framework\n\n### Deploy OLM and Catalog Operator\n\n```\n$ git clone https://github.com/operator-framework/operator-lifecycle-manager.git\n$ kubectl create -f deploy/upstream/manifests/latest/\n```\nRefer to [Install document](https://github.com/operator-framework/operator-lifecycle-manager/blob/master/Documentation/install/install.md) for details.\n\n### Deploy Marketplace Operator\n\n```\n$ git clone https://github.com/operator-framework/operator-marketplace.git\n$ kubectl apply -f deploy/upstream\n```\nRefer to [README.md](https://github.com/operator-framework/operator-marketplace/blob/master/README.md) for details.\n\n> **Note:** If you need to visualize *OperatorHub* UI in OKD, please deploy market operator in *openshift-marketplace* namespace.\n\n### Visualize OperatorHub in OKD 3.11\n\n1. Prepare an OKD 3.11 cluster\n2. Edit openshift-console deployment to replace ```image: docker.io/openshift/origin-console:v3.11``` to ```image: docker.io/openshift/origin-console:v4.0.0```\n```\n$ oc edit deployment console -n openshift-console\n```\n3. login to openshift GUI at ```/operatorhub/all-namespaces```\n\n## Workflow\n\n1. Operator developer prepares all [CSV](https://github.com/operator-framework/operator-lifecycle-manager/blob/master/Documentation/design/building-your-csv.md), CRD, and Package yamls.\n2. Operator developer uses [```operator-courier```](https://github.com/operator-framework/operator-courier/#usage) to verify and push operator bundle to the Quay application repository.\n3. Cluster admin creates [```OperatorSource```](https://github.com/operator-framework/operator-marketplace#description) CR to add the operator source into cluster. (All operators under the same Quay namespace share the same ```OperatorSource```)\n4. Cluster admin creates [```CatalogSourceConfig```](https://github.com/operator-framework/operator-marketplace/blob/master/README.md#description) to enable selected operators in ```OperatorSource``` to ```CatalogSource```\n5. Cluster admin creates [```OperatorGroup```](https://github.com/operator-framework/operator-lifecycle-manager/blob/master/Documentation/design/operatorgroups.md) for target namespaces where operators are going to be installed into.\n6. Cluster admin creates ```Subscription``` to subscribe operators.\n7. *Catalog Operator* will reconcile to ```Subscription``` and create ```InstallPlan```.\n8. *Catalog Operator* will reconcile to ```InstallPlan``` and create corresponding ```ClusterServiceVersion```.\n9. *OLM Operator* reconciles to ```ClusterServiceVersion``` and make operators started.\n10. User creates CRs defined by operators to create services.\n\n### Examples\n\n1. Import **operatorsource** provided by Federator.ai\n    ```\n    $ kubectl apply -f prophetstor-operatorsource.yaml\n    $ cat prophetstor-operatorsource.yaml\n    apiVersion: operators.coreos.com/v1\n    kind: OperatorSource\n    metadata:\n      name: prophetstor-operators\n      namespace: openshift-marketplace\n    spec:\n      type: appregistry\n      endpoint: https://quay.io/cnr\n      registryNamespace: prophetstor\n      displayName: \"prophetstor Operators\"\n      publisher: \"prophetstor\"\n    ```\n\n    You should see the FederatorAI Operator in the UI.\n    ![operatorhub](./img/operatorsource.png)\n\n2. Subscribe Federator.ai operator\n    ![subscribe](./img/subscribeoperator.png)\n\n3. Configure namespace and approval strategy\n    ![config subscription](./img/subscribesetting.png)\n\n4. (Optional) Approve Federator.ai operator **InstallPlan** if you toggle *Manual* approval strategy when you were subscribing Federator.ai operator\n\n5. Check subscription status\n    ![subscription status](./img/subscribestatus.png)\n\n6. Check installed ClusterServiceVersion\n    ![csv](./img/csv.png)\n\n7. When Federator.ai **ClusterServiceVersion** of Federator.ai operator is created, we can create an **AlamedaService** CR to start deploy Alameda\n    ![create alamedaservice](./img/createcr.png)\n\n"
    },
    {}
]

